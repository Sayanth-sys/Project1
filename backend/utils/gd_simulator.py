import time
import random
import concurrent.futures
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from typing import List, Dict, Optional
import json
from gtts import gTTS
import io
import base64
import os
import tempfile
from dotenv import load_dotenv
import wave
from vosk import Model, KaldiRecognizer
import subprocess
import shutil
import whisper

# -------------------------
# üîê Gemini Setup
# -------------------------
load_dotenv()
API_KEY = os.getenv("API_KEY")

# ‚úÖ IMPROVED: Better FFmpeg detection
def setup_ffmpeg():
    """Detect and setup FFmpeg path"""
    # Check if ffmpeg is already in PATH
    if shutil.which("ffmpeg"):
        print("[INFO] ‚úÖ FFmpeg found in system PATH")
        return True
    
    # Common FFmpeg installation paths
    common_paths = [
        r"C:\ffmpeg\bin",
        r"C:\Program Files\ffmpeg\bin",
        r"/usr/local/bin",
        r"/usr/bin"
    ]
    
    for path in common_paths:
        if os.path.exists(os.path.join(path, "ffmpeg.exe" if os.name == 'nt' else "ffmpeg")):
            os.environ["PATH"] += os.pathsep + path
            print(f"[INFO] ‚úÖ FFmpeg found at: {path}")
            return True
    
    print("[ERROR] ‚ùå FFmpeg not found!")
    print("[INFO] üí° Install FFmpeg:")
    print("  Windows: Download from https://ffmpeg.org/download.html")
    print("  Linux: sudo apt-get install ffmpeg")
    print("  Mac: brew install ffmpeg")
    return False

ffmpeg_available = setup_ffmpeg()

genai.configure(api_key=API_KEY)
chat_model = genai.GenerativeModel("models/gemini-2.5-flash-preview-09-2025")

# -------------------------
# üé§ Whisper Model Setup
# -------------------------
print("[INFO] üß† Loading Whisper model...")
try:
    whisper_model = whisper.load_model("base")
    print("[INFO] ‚úÖ Whisper model loaded successfully")
except Exception as e:
    whisper_model = None
    print(f"[ERROR] ‚ùå Failed to load Whisper model: {e}")

# -------------------------
# üé§ Vosk Model Setup (Offline Speech Recognition - Fallback)
# -------------------------
VOSK_MODEL_PATH = "vosk-model-small-en-us-0.15"
print("[INFO] Loading Vosk speech recognition model...")
if os.path.exists(VOSK_MODEL_PATH):
    try:
        vosk_model = Model(VOSK_MODEL_PATH)
        print(f"[INFO] ‚úÖ Vosk model loaded from: {VOSK_MODEL_PATH}")
    except Exception as e:
        vosk_model = None
        print(f"[ERROR] ‚ùå Failed to load Vosk model: {e}")
else:
    vosk_model = None
    print(f"[WARNING] ‚ö†Ô∏è Vosk model not found at: {VOSK_MODEL_PATH}")
    print("[WARNING] üí° Download from: https://alphacephei.com/vosk/models")
    print("[WARNING] üí° Extract to project root and rename to 'vosk-model-small-en-us-0.15'")

# -------------------------
# üß† Embedding Model
# -------------------------
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# -------------------------
# üßç Personas
# -------------------------
PERSONAS = [
    "logical and fact-driven",
    "supportive and encouraging",
    "critical but respectful",
    "creative and optimistic",
    "cautious and balanced",
    "structured and methodical"
]

# -------------------------
# üß© Helper Functions
# -------------------------
def safe_generate(prompt, timeout=60):
    """Call Gemini safely with timeout and debugging."""
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(chat_model.generate_content, prompt)
        try:
            response = future.result(timeout=timeout)
            text = response.text.strip() if response and hasattr(response, "text") else "[No response from Gemini]"
            print(f"[DEBUG] Generated response: {text}")
            return text
        except concurrent.futures.TimeoutError:
            print("[TIMEOUT] Gemini took too long for this prompt.")
            return "[Timeout error]"
        except Exception as e:
            print(f"[ERROR] During Gemini generation: {e}")
            return f"[Error: {e}]"

def text_to_audio_base64(text, agent_name):
    """Convert text to audio with agent-specific accent."""
    try:
        accent_map = {
            "Agent 1": "co.in",
            "Agent 2": "com",
            "Agent 3": "co.uk",
            "Agent 4": "com.au"
        }
        tld = accent_map.get(agent_name, "com")
        tts = gTTS(text, lang="en", tld=tld, slow=False)
        audio_bytes = io.BytesIO()
        tts.write_to_fp(audio_bytes)
        audio_bytes.seek(0)
        return base64.b64encode(audio_bytes.read()).decode("utf-8")
    except Exception as e:
        print(f"[ERROR] TTS failed: {e}")
        return None

def transcribe_audio_whisper(audio_file):
    """
    ‚úÖ PRIMARY: Transcribe audio using Whisper (more accurate)
    """
    temp_path = None
    try:
        # Check prerequisites
        if whisper_model is None:
            print("[ERROR] ‚ùå Whisper model not loaded")
            return None
        
        if not ffmpeg_available:
            print("[ERROR] ‚ùå FFmpeg not available (required for Whisper)")
            return None
        
        # Read uploaded audio
        audio_data = audio_file.read()
        audio_file.seek(0)
        print(f"[DEBUG] üé§ Received audio: {len(audio_data)} bytes")
        
        if len(audio_data) < 1000:
            print("[ERROR] ‚ùå Audio file too small (likely empty recording)")
            return None
        
        # Create temp file
        temp_dir = tempfile.gettempdir()
        timestamp = int(time.time() * 1000)
        temp_path = os.path.join(temp_dir, f"whisper_audio_{timestamp}.webm")
        
        with open(temp_path, 'wb') as f:
            f.write(audio_data)
        
        print(f"[DEBUG] üíæ Saved audio to: {temp_path}")
        print("[DEBUG] üß† Transcribing with Whisper...")
        
        # Transcribe with Whisper
        result = whisper_model.transcribe(temp_path)
        transcribed_text = result["text"].strip()
        
        if transcribed_text:
            print(f"[SUCCESS] ‚úÖ Whisper transcribed: '{transcribed_text}'")
            return transcribed_text
        else:
            print("[ERROR] ‚ùå Whisper returned empty transcription")
            return None
            
    except Exception as e:
        print(f"[ERROR] ‚ùå Whisper transcription failed: {e}")
        import traceback
        print(f"[ERROR] Full traceback:\n{traceback.format_exc()}")
        return None
    finally:
        # Cleanup temp file
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                print(f"[DEBUG] üóëÔ∏è Cleaned up: {temp_path}")
            except Exception as e:
                print(f"[WARNING] ‚ö†Ô∏è Could not delete {temp_path}: {e}")

def transcribe_audio_vosk(audio_file):
    """
    ‚úÖ FALLBACK: Enhanced transcription with Vosk (used if Whisper fails)
    """
    webm_path = None
    wav_path = None
    try:
        # Check prerequisites
        if vosk_model is None:
            print("[ERROR] ‚ùå Vosk model not loaded")
            return None
        
        if not ffmpeg_available:
            print("[ERROR] ‚ùå FFmpeg not available")
            return None
        
        # Read uploaded audio
        audio_data = audio_file.read()
        audio_file.seek(0)
        print(f"[DEBUG] üé§ Received audio: {len(audio_data)} bytes")
        
        if len(audio_data) < 1000:
            print("[ERROR] ‚ùå Audio file too small (likely empty recording)")
            return None
        
        # Create temp directory if it doesn't exist
        temp_dir = tempfile.gettempdir()
        print(f"[DEBUG] üìÅ Using temp directory: {temp_dir}")
        
        # Save WebM temporarily with unique filename
        timestamp = int(time.time() * 1000)
        webm_path = os.path.join(temp_dir, f"recording_{timestamp}.webm")
        wav_path = os.path.join(temp_dir, f"recording_{timestamp}.wav")
        
        with open(webm_path, 'wb') as f:
            f.write(audio_data)
        
        print(f"[DEBUG] üíæ Saved WebM to: {webm_path}")
        
        # üîÑ Convert WebM ‚Üí WAV using FFmpeg
        print("[DEBUG] üîÑ Converting WebM ‚Üí WAV using FFmpeg...")
        ffmpeg_cmd = [
            "ffmpeg",
            "-y",  # Overwrite output file
            "-i", webm_path,  # Input file
            "-ac", "1",  # Mono audio
            "-ar", "16000",  # 16kHz sample rate
            "-sample_fmt", "s16",  # 16-bit PCM
            "-f", "wav",  # Force WAV format
            wav_path
        ]
        
        print(f"[DEBUG] Running: {' '.join(ffmpeg_cmd)}")
        result = subprocess.run(
            ffmpeg_cmd,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode != 0:
            print(f"[ERROR] ‚ùå FFmpeg failed with code {result.returncode}")
            print(f"[ERROR] FFmpeg stderr: {result.stderr}")
            return None
        
        print(f"[DEBUG] ‚úÖ FFmpeg conversion successful")
        
        # Verify WAV file exists and has content
        if not os.path.exists(wav_path):
            print("[ERROR] ‚ùå WAV file was not created")
            return None
        
        wav_size = os.path.getsize(wav_path)
        print(f"[DEBUG] üìä WAV file size: {wav_size} bytes")
        
        if wav_size < 1000:
            print("[ERROR] ‚ùå WAV file too small (conversion likely failed)")
            return None
        
        # Open WAV file
        wf = wave.open(wav_path, "rb")
        print(f"[DEBUG] üìä WAV properties:")
        print(f"  - Channels: {wf.getnchannels()}")
        print(f"  - Sample width: {wf.getsampwidth()}")
        print(f"  - Frame rate: {wf.getframerate()}")
        print(f"  - Frames: {wf.getnframes()}")
        print(f"  - Duration: {wf.getnframes() / wf.getframerate():.2f}s")
        
        # Validate WAV format
        if wf.getnchannels() != 1 or wf.getsampwidth() != 2:
            print("[ERROR] ‚ùå Invalid WAV format for Vosk")
            print(f"  Expected: 1 channel, 2 bytes sample width")
            print(f"  Got: {wf.getnchannels()} channels, {wf.getsampwidth()} bytes")
            wf.close()
            return None
        
        # Create recognizer
        print("[DEBUG] üéØ Creating Vosk recognizer...")
        rec = KaldiRecognizer(vosk_model, wf.getframerate())
        rec.SetWords(True)
        
        result_text = ""
        frames_processed = 0
        
        print("[DEBUG] üîç Starting transcription...")
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            
            frames_processed += 1
            
            if rec.AcceptWaveform(data):
                part_result = json.loads(rec.Result())
                part_text = part_result.get("text", "")
                if part_text:
                    print(f"[DEBUG] üìù Partial result: '{part_text}'")
                    result_text += part_text + " "
        
        # Get final result
        final_result = json.loads(rec.FinalResult())
        final_text = final_result.get("text", "")
        if final_text:
            print(f"[DEBUG] üìù Final result: '{final_text}'")
            result_text += final_text
        
        wf.close()
        
        result_text = result_text.strip()
        print(f"[DEBUG] üìä Processed {frames_processed} frame chunks")
        
        if result_text:
            print(f"[SUCCESS] ‚úÖ Vosk transcribed: '{result_text}'")
            return result_text
        else:
            print("[ERROR] ‚ùå No speech detected in audio")
            print("[INFO] üí° Possible causes:")
            print("  - Audio too quiet")
            print("  - Background noise too loud")
            print("  - Recording too short")
            print("  - Microphone not working properly")
            return None
            
    except subprocess.TimeoutExpired:
        print("[ERROR] ‚ùå FFmpeg conversion timeout (>30s)")
        return None
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] ‚ùå FFmpeg failed: {e}")
        return None
    except Exception as e:
        print(f"[ERROR] ‚ùå Vosk transcription failed: {e}")
        import traceback
        print(f"[ERROR] Full traceback:\n{traceback.format_exc()}")
        return None
    finally:
        # Cleanup temp files
        for path in [webm_path, wav_path]:
            if path and os.path.exists(path):
                try:
                    os.remove(path)
                    print(f"[DEBUG] üóëÔ∏è Cleaned up: {path}")
                except Exception as e:
                    print(f"[WARNING] ‚ö†Ô∏è Could not delete {path}: {e}")

def transcribe_audio(audio_file):
    """
    ‚úÖ HYBRID: Try Whisper first (more accurate), fallback to Vosk
    """
    print("[INFO] üéØ Attempting transcription with Whisper (primary)...")
    
    # Try Whisper first
    if whisper_model is not None:
        result = transcribe_audio_whisper(audio_file)
        if result:
            return result
        print("[WARNING] ‚ö†Ô∏è Whisper failed, trying Vosk fallback...")
    else:
        print("[WARNING] ‚ö†Ô∏è Whisper not available, using Vosk...")
    
    # Fallback to Vosk
    if vosk_model is not None:
        return transcribe_audio_vosk(audio_file)
    
    print("[ERROR] ‚ùå Both Whisper and Vosk unavailable")
    return None

# -------------------------
# üó£Ô∏è Agent Class
# -------------------------
class Agent:
    def __init__(self, name, persona):
        self.name = name
        self.persona = persona
    
    def prepare_prompt(self, topic, utterances, is_first=False, human_just_spoke=False):
        full_discussion = "\n".join(
            f"- {u['agent']}: {u['text']}" for u in utterances
        )
        
        last_remark = utterances[-1]["text"] if utterances else ""
        last_speaker = utterances[-1]["agent"] if utterances else ""
        
        if is_first and not utterances:
            return f"""
You are {self.name}, a participant in a group discussion.
Your style: {self.persona}.

Topic: {topic}

You are the first to speak. Start naturally with your viewpoint (under 80 words).

Speak like a normal college student in a group discussion.

Rules:
- Keep it simple and conversational
- Use short sentences
- Avoid formal or academic language
- It's okay to sound slightly casual
- Say just one clear point
- Do NOT explain too much
- 25‚Äì50 words only

You may naturally use phrases like: "I think", "Honestly", "I feel", "To be fair", "I agree, but"
"""
        
        human_context = ""
        if human_just_spoke and last_speaker == "You":
            human_context = f"\n\nIMPORTANT: A human participant (labeled 'You') just shared their view: \"{last_remark}\"\nRespond directly to their point, acknowledging what they said."
        
        return f"""
You are {self.name}, a participant in a group discussion.
Your style: {self.persona}.

Topic: {topic}

Discussion so far (for your awareness only, do NOT summarize or repeat it):
{full_discussion or "No prior remarks."}

Most recent comment from {last_speaker}: "{last_remark}"{human_context}

Respond naturally in under 80 words, staying consistent with your persona.

Speak like a normal college student in a group discussion.

Rules:
- Keep it simple and conversational
- Use short sentences
- Avoid formal or academic language
- It's okay to sound slightly casual
- Say just one clear point
- Do NOT explain too much
- 25‚Äì50 words only
{"- Acknowledge the human's point naturally if they just spoke" if human_just_spoke else ""}

You may naturally use phrases like: "I think", "Honestly", "I feel", "To be fair", "I agree, but"
"""
    
    def generate_response(self, prompt):
        print(f"[DEBUG] {self.name} is generating response...")
        text = safe_generate(prompt)
        print(f"[DEBUG] {self.name} finished response.")
        return text

# -------------------------
# ‚öôÔ∏è FastAPI setup
# -------------------------
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------------
# üóÇÔ∏è In-memory simulations
# -------------------------
SIMULATIONS = {}

class SimulationRequest(BaseModel):
    topic: str
    num_agents: int = 4
    rounds: int = 2
    human_participant: bool = True

class HumanInputRequest(BaseModel):
    text: Optional[str] = None

# -------------------------
# üöÄ API Endpoints
# -------------------------
@app.post("/start_simulation")
def start_simulation(req: SimulationRequest):
    print(f"\n{'='*60}")
    print(f"üé¨ STARTING NEW SIMULATION")
    print(f"{'='*60}")
    print(f"üìã Topic: {req.topic}")
    print(f"üë• Agents: {req.num_agents}")
    print(f"üîÑ Rounds: {req.rounds}")
    print(f"üé§ Human Participant: {req.human_participant}")
    
    selected_personas = random.sample(PERSONAS, req.num_agents)
    agents = [Agent(f"Agent {i+1}", persona) for i, persona in enumerate(selected_personas)]
    
    sim_id = str(len(SIMULATIONS) + 1)
    SIMULATIONS[sim_id] = {
        "topic": req.topic,
        "agents": agents,
        "utterances": [],
        "current_round": 0,
        "total_rounds": req.rounds,
        "human_participant": req.human_participant,
        "awaiting_human": False
    }
    
    print(f"\n‚úÖ Simulation ID: {sim_id}")
    print("üë• Agent Lineup:")
    for a in agents:
        print(f"  ‚Ä¢ {a.name}: {a.persona}")
    print(f"{'='*60}\n")
    
    agent_list = [{"name": agent.name, "persona": agent.persona} for agent in agents]
    return {"simulation_id": sim_id, "agents": agent_list}

@app.post("/submit_human_input/{sim_id}")
async def submit_human_input(sim_id: str, audio: Optional[UploadFile] = File(None), text: Optional[str] = None):
    """Accept human input via voice or text."""
    print(f"\n{'='*60}")
    print(f"üë§ HUMAN INPUT RECEIVED")
    print(f"{'='*60}")
    
    if sim_id not in SIMULATIONS:
        print("‚ùå ERROR: Simulation ID not found")
        return {"error": "Simulation ID not found."}
    
    sim = SIMULATIONS[sim_id]
    
    # Get human input
    human_text = None
    if audio:
        print("üé§ Processing voice input (Whisper primary, Vosk fallback)...")
        human_text = transcribe_audio(audio.file)
    elif text:
        print(f"‚å®Ô∏è Processing text input: '{text}'")
        human_text = text
    
    if not human_text:
        error_msg = "Could not transcribe audio. "
        if not whisper_model and not vosk_model:
            error_msg += "No speech recognition models loaded. "
        if not ffmpeg_available:
            error_msg += "FFmpeg not available. "
        error_msg += "Please use text input instead."
        print(f"‚ùå ERROR: {error_msg}")
        return {"error": error_msg}
    
    print(f"\nüí¨ HUMAN SAID: \"{human_text}\"")
    print(f"{'='*60}\n")
    
    # Add human utterance to discussion
    sim["utterances"].append({
        "agent": "You",
        "text": human_text,
        "audio": None
    })
    sim["awaiting_human"] = False
    
    return {"success": True, "transcribed_text": human_text}

@app.post("/next_round/{sim_id}")
def next_round(sim_id: str):
    if sim_id not in SIMULATIONS:
        return {"error": "Simulation ID not found."}
    
    sim = SIMULATIONS[sim_id]
    
    if sim["current_round"] >= sim["total_rounds"]:
        return {"message": "Simulation completed.", "utterances": sim["utterances"]}
    
    utterances = sim["utterances"]
    agents = sim["agents"]
    topic = sim["topic"]
    human_participant = sim["human_participant"]
    
    speaking_order = []
    if human_participant:
        all_speakers = agents.copy()
        human_position = random.randint(0, len(all_speakers))
        speaking_order = all_speakers[:human_position] + ["HUMAN_TURN"] + all_speakers[human_position:]
    else:
        speaking_order = random.sample(agents, len(agents))
    
    print(f"\n{'='*60}")
    print(f"üîÑ ROUND {sim['current_round'] + 1} STARTING")
    print(f"{'='*60}")
    print(f"üì¢ Speaking order: {[a.name if isinstance(a, Agent) else 'üë§ You (Human)' for a in speaking_order]}")
    print(f"{'='*60}\n")
    
    def generate():
        human_just_spoke = False
        for i, speaker in enumerate(speaking_order):
            if speaker == "HUMAN_TURN":
                print(f"\nüé§ {'='*50}")
                print(f"  YOUR TURN TO SPEAK!")
                print(f"  {'='*50}\n")
                
                yield f"data: {json.dumps({'type': 'human_turn'})}\n\n"
                sim["awaiting_human"] = True
                
                max_wait = 120
                waited = 0
                while sim["awaiting_human"] and waited < max_wait:
                    time.sleep(1)
                    waited += 1
                
                if waited >= max_wait:
                    print("‚è∞ TIMEOUT: Human took too long to respond")
                    yield f"data: {json.dumps({'type': 'error', 'message': 'Timeout waiting for human input'})}\n\n"
                    continue
                
                human_utterance = sim["utterances"][-1]
                print(f"‚úÖ Human response submitted: \"{human_utterance['text']}\"")
                yield f"data: {json.dumps({'type': 'human_response', 'text': human_utterance['text']})}\n\n"
                human_just_spoke = True
            else:
                agent = speaker
                print(f"\nüí≠ {agent.name} is thinking...")
                yield f"data: {json.dumps({'type': 'thinking', 'agent': agent.name})}\n\n"
                
                prompt = agent.prepare_prompt(
                    topic,
                    utterances,
                    is_first=(sim["current_round"] == 0 and i == 0),
                    human_just_spoke=human_just_spoke
                )
                text = agent.generate_response(prompt)
                audio_base64 = text_to_audio_base64(text, agent.name)
                
                data = {"agent": agent.name, "text": text, "audio": audio_base64}
                utterances.append(data)
                
                print(f"üó£Ô∏è {agent.name}: \"{text}\"")
                yield f"data: {json.dumps({'type': 'response', 'agent': agent.name, 'text': text, 'audio': audio_base64})}\n\n"
                human_just_spoke = False
        
        sim["current_round"] += 1
        print(f"\nüéâ Round {sim['current_round']} completed!\n")
        yield f"data: {json.dumps({'type': 'complete', 'round': sim['current_round']})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

@app.get("/simulation_status/{sim_id}")
def get_status(sim_id: str):
    """Get current simulation status."""
    if sim_id not in SIMULATIONS:
        return {"error": "Simulation ID not found."}
    
    sim = SIMULATIONS[sim_id]
    return {
        "current_round": sim["current_round"],
        "total_rounds": sim["total_rounds"],
        "awaiting_human": sim["awaiting_human"],
        "utterances_count": len(sim["utterances"]),
        "whisper_loaded": whisper_model is not None,
        "vosk_loaded": vosk_model is not None,
        "ffmpeg_available": ffmpeg_available
    }

@app.get("/health")
def health_check():
    """Check if all required components are available."""
    status_msg = []
    
    if whisper_model is not None:
        status_msg.append("Whisper (primary)")
    if vosk_model is not None:
        status_msg.append("Vosk (fallback)")
    if not whisper_model and not vosk_model:
        status_msg.append("No STT available")
    
    return {
        "status": "ok",
        "speech_recognition": " + ".join(status_msg) if status_msg else "unavailable",
        "whisper_loaded": whisper_model is not None,
        "vosk_model_loaded": vosk_model is not None,
        "ffmpeg_available": ffmpeg_available,
        "vosk_model_path": VOSK_MODEL_PATH if os.path.exists(VOSK_MODEL_PATH) else "Not found"
    }